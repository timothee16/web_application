{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import sqlite3\n",
    "import gzip\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Define the paths to form the QSL database\n",
    "base_path = \"./\" \n",
    "folders = ['hosp', 'icu', 'note']\n",
    "db_path = \"mimic.db\"\n",
    "\n",
    "# Connect to SQLite database\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Iterate through each folder and process .csv.gz files\n",
    "for folder in folders: \n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    \n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".csv.gz\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            table_name = file.replace(\".csv.gz\", \"\")\n",
    "\n",
    "            chunk_size = 1000000\n",
    "            for chunk in pd.read_csv(file_path, compression='gzip', chunksize= chunk_size):\n",
    "                chunk.to_sql(table_name, conn, if_exists='append', index=False)\n",
    "            \n",
    "            print(f'Finished processing {file} into table {table_name}')\n",
    "\n",
    "# Close the connection\n",
    "conn.close()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path= '../database/mimic.db'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the SQLite database to query the files\n",
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patients\n",
    "patients = pd.read_sql_query(\"\"\"SELECT subject_id, gender, anchor_age FROM patients\"\"\", conn)\n",
    "\n",
    "# LAdmissions - filtered by patients' number of admissions\n",
    "admissions = pd.read_sql_query(\"\"\"SELECT subject_id, hadm_id, admittime, dischtime FROM admissions WHERE subject_id IN (SELECT subject_id FROM admissions GROUP BY subject_id HAVING COUNT(hadm_id) = 2) ORDER BY subject_id\"\"\", conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge admissions with patients\n",
    "admissions_patients = admissions.merge(patients, on= 'subject_id', how='left')\n",
    "display(admissions_patients.head())\n",
    "print(admissions_patients['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the d_icd_diagnoses into a database\n",
    "query_d_icd_diagnoses = \"\"\"SELECT * FROM d_icd_diagnoses;\"\"\"\n",
    "d_icd_diagnoses_df = pd.read_sql_query(query_d_icd_diagnoses, conn)\n",
    "\n",
    "# Load the diagnoses_icd into a database\n",
    "query_diagnoses_icd = \"SELECT * FROM diagnoses_icd;\"\n",
    "diagnoses_icd_df = pd.read_sql_query(query_diagnoses_icd, conn)\n",
    "\n",
    "# 1. Merge the two databases into a diagnoses database\n",
    "diagnoses = diagnoses_icd_df.merge(d_icd_diagnoses_df, on=['icd_code', 'icd_version'], how = 'inner')\n",
    "\n",
    "# 2. Count the frequent diagnoses\n",
    "frequent_diagnoses = (\n",
    "    diagnoses_icd_df.groupby(['icd_code', 'icd_version'], as_index=False)\n",
    "    .size()\n",
    "    .rename(columns={'size': 'count'})\n",
    ")\n",
    "frequent_diagnoses = frequent_diagnoses[frequent_diagnoses['count'] >= 10000]   # filter frequent diagnoses\n",
    "\n",
    "# 3. Filter the diagnoses table with the frequent diagnoses\n",
    "diagnoses_df = diagnoses.merge(frequent_diagnoses[['icd_code', 'icd_version']], on=['icd_code', 'icd_version'], how = 'inner')\n",
    "display(diagnoses_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SQL query\n",
    "'''query = \"\"\"\n",
    "SELECT COUNT(*) AS total_drugs_with_multiple_units\n",
    "FROM (\n",
    "    SELECT drug\n",
    "    FROM prescriptions\n",
    "    GROUP BY drug\n",
    "    HAVING COUNT(DISTINCT LOWER(dose_unit_rx)) > 1\n",
    ");\n",
    "\"\"\"\n",
    "# Execute the query and fetch the results\n",
    "cursor.execute(query)\n",
    "result = cursor.fetchone()\n",
    "\n",
    "# Print the total number of drugs with multiple dose units\n",
    "print(f\"Total drugs with multiple dose units: {result[0]}\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the prescriptions table into a dataframe\n",
    "query_prescriptions = \"\"\"SELECT subject_id, hadm_id, drug, dose_val_rx, dose_unit_rx FROM prescriptions;\"\"\"\n",
    "prescriptions = pd.read_sql_query(query_prescriptions, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert \"drug\" and \"dose_unit_rx\" into lowercase\n",
    "prescriptions['drug'] = prescriptions['drug'].str.lower()\n",
    "prescriptions['dose_unit_rx'] = prescriptions['dose_unit_rx'].str.lower()\n",
    "\n",
    "# Filter drugs with consistent drug units\n",
    "valid_drugs = prescriptions.groupby('drug')['dose_unit_rx'].nunique()\n",
    "consistent_drugs = valid_drugs[valid_drugs == 1].index  # drugs with only one single unit\n",
    "consistent_presc_df = prescriptions[prescriptions['drug'].isin(consistent_drugs)]  # filter the dataset\n",
    "\n",
    "# Remove the low-frequency drugs \n",
    "drug_counts = consistent_presc_df['drug'].value_counts()\n",
    "frequent_drugs = drug_counts[drug_counts >= 1000].index  # Only drugs with at least 1000 occurrences\n",
    "\n",
    "# final dataset\n",
    "prescriptions_df = consistent_presc_df[consistent_presc_df['drug'].isin(frequent_drugs)]\n",
    "prescriptions_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_df = admissions_patients.copy()\n",
    "diag_df = diagnoses_df.copy()\n",
    "prescr_df = prescriptions_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate length of stay and add the number of stay\n",
    "adm_df['length_of_stay'] = (pd.to_datetime(adm_df['dischtime']) - pd.to_datetime(adm_df['admittime'])).dt.days\n",
    "adm_df['stay'] = adm_df.groupby('subject_id')['hadm_id'].transform(lambda x: x.rank(method='dense'))\n",
    "adm_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column for first LOS \n",
    "first_stay_lengths = adm_df[adm_df['stay'] == 1].groupby('subject_id')['length_of_stay'].first().reset_index()\n",
    "first_stay_lengths = first_stay_lengths.rename(columns={'length_of_stay': 'lengths_of_1st_admission'})\n",
    "\n",
    "# Column for second LOS\n",
    "second_stay_lengths = adm_df[adm_df['stay'] == 2].groupby('subject_id')['length_of_stay'].first().reset_index()\n",
    "second_stay_lengths = second_stay_lengths.rename(columns={'length_of_stay': 'lengths_of_2nd_admission'})\n",
    "\n",
    "# Merge \n",
    "stays = first_stay_lengths.merge(second_stay_lengths, on='subject_id')\n",
    "display(first_stay_lengths.head())\n",
    "display(second_stay_lengths.head())\n",
    "display(stays.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the stays with the admissions dataframe\n",
    "adm_df = adm_df.merge(stays, on='subject_id')\n",
    "adm_df = adm_df.drop(columns =['length_of_stay', 'stay'])\n",
    "adm_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the target variable \"lengths_of_2nd_admission\" as binary for classification\n",
    "adm_df['lengths_of_2nd_admission'] = (adm_df['lengths_of_2nd_admission'] >= 3).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode diagnoses per subject_id\n",
    "diagnosis_pivot = diag_df.pivot_table(index='subject_id', columns='long_title', aggfunc='size', fill_value=0)\n",
    "diagnosis_pivot = (diagnosis_pivot > 0).astype(int)  # set as binary values\n",
    "diagnosis_pivot.reset_index(inplace=True)\n",
    "diagnosis_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the dose column is numeric\n",
    "prescr_df['dose_val_rx'] = pd.to_numeric(prescr_df['dose_val_rx'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select and filter the 68 medications\n",
    "top_medications = prescr_df['drug'].value_counts().head(68).index\n",
    "filtered_med_df = prescr_df[prescr_df['drug'].isin(top_medications)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table for sum of doses per drug\n",
    "sum_dose_pivot = filtered_med_df.pivot_table(index='subject_id', columns='drug', values='dose_val_rx', aggfunc='sum', fill_value=0)\n",
    "sum_dose_pivot.columns = [f'{col} Sum' for col in sum_dose_pivot.columns]  # Rename columns\n",
    "\n",
    "# Pivot table for mean dose per drug\n",
    "avg_dose_pivot = filtered_med_df.pivot_table(index='subject_id', columns='drug', values='dose_val_rx', aggfunc='mean', fill_value=0)\n",
    "avg_dose_pivot.columns = [f'{col} Average' for col in avg_dose_pivot.columns]  # Rename columns\n",
    "\n",
    "display(sum_dose_pivot.head())\n",
    "display(avg_dose_pivot.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two dose doses datasets\n",
    "med_pivot = sum_dose_pivot.merge(avg_dose_pivot, on='subject_id', how='left')\n",
    "med_pivot.reset_index(inplace=True)\n",
    "med_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge admissions with prescriptions\n",
    "final_df = adm_df.merge(med_pivot, on = ['subject_id'], how='inner')\n",
    "display(final_df.head(3))\n",
    "print(final_df['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataset with diagnoses\n",
    "final_df = final_df.merge(diagnosis_pivot, on = ['subject_id'], how='inner')\n",
    "display(final_df.head(3))\n",
    "print(final_df['subject_id'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(db_path)\n",
    "cursor = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary table from final_df for subject_id and hadm_id\n",
    "final_df[['subject_id', 'hadm_id', 'admittime']].to_sql('temp_los', conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the discharge notes from the discharge table ONLY for the first admisison\n",
    "discharge_text = pd.read_sql_query(\"\"\"\n",
    "    SELECT d.subject_id, d.hadm_id, d.text\n",
    "    FROM discharge d\n",
    "    JOIN (\n",
    "        SELECT subject_id, hadm_id\n",
    "        FROM temp_los\n",
    "        WHERE admittime = (SELECT MIN(admittime) \n",
    "                           FROM temp_los t \n",
    "                           WHERE t.subject_id = temp_los.subject_id)\n",
    "    ) first_admissions \n",
    "    ON d.subject_id = first_admissions.subject_id \n",
    "    AND d.hadm_id = first_admissions.hadm_id\n",
    "\"\"\", conn)\n",
    "discharge_text.drop(columns='hadm_id', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge discharge note text into final_df \n",
    "final_df = final_df.merge(discharge_text, on=['subject_id'], how='left')\n",
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adm_type = pd.read_sql_query(\"\"\"\n",
    "    SELECT subject_id, hadm_id,\n",
    "           admission_type \n",
    "    FROM admissions\n",
    "    GROUP BY subject_id, hadm_id\n",
    "\"\"\", conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the DRG data into final_df\n",
    "admtype_pivot =  adm_type.pivot_table(index='subject_id', columns = 'admission_type', aggfunc='size', fill_value=0)\n",
    "admtype_pivot = (admtype_pivot > 0).astype(int)\n",
    "final_df = final_df.merge(admtype_pivot, on=['subject_id'], how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drg_sums = pd.read_sql_query(\"\"\"\n",
    "    SELECT subject_id, hadm_id,\n",
    "           SUM(drg_severity) AS sum_drg_severity,\n",
    "           SUM(drg_mortality) AS sum_drg_mortality\n",
    "    FROM drgcodes\n",
    "    GROUP BY subject_id, hadm_id\n",
    "\"\"\", conn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a df with the DRG values for the admission dates\n",
    "first_hadm = final_df.groupby('subject_id')['hadm_id'].first().reset_index()\n",
    "drg_sums.drop(columns='subject_id', inplace=True)\n",
    "merged_severity = first_hadm.merge(drg_sums, on='hadm_id', how='left')\n",
    "merged_severity.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the values with the final dataset\n",
    "final_df = final_df.merge(merged_severity[['subject_id', 'sum_drg_severity', 'sum_drg_mortality']], on='subject_id', how='left')\n",
    "\n",
    "# fill null values with 0 (scores from 1 to 4)\n",
    "final_df['sum_drg_severity'] = final_df['sum_drg_severity'].fillna(0)\n",
    "final_df['sum_drg_mortality'] = final_df['sum_drg_mortality'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_weights = pd.read_sql_query(\"\"\"\n",
    "    SELECT i.subject_id, i.hadm_id, i.patientweight\n",
    "    FROM inputevents i\n",
    "\"\"\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average standard deviation for weight across all patients\n",
    "std_per_patient = input_weights.groupby('subject_id')['patientweight'].std()\n",
    "average_std = std_per_patient.mean()\n",
    "average_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the dataset to have only one row per subject_id\n",
    "df_aggregated = input_weights.groupby('subject_id')[\"patientweight\"].mean().reset_index()\n",
    "\n",
    "# Now you can merge it with your other dataset\n",
    "final_df = final_df.merge(df_aggregated, on='subject_id', how='left')\n",
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integrate with the weight from omr table\n",
    "omr_weights = pd.read_sql_query(\"\"\"\n",
    "    SELECT o.subject_id, o.result_value AS patientweight_omr\n",
    "    FROM omr o\n",
    "    WHERE o.result_name = 'Weight (Lbs)'\n",
    "\"\"\", conn)\n",
    "omr_weights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "omr_weights['patientweight_omr'] = pd.to_numeric(omr_weights['patientweight_omr'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average standard deviation for weight across all patients\n",
    "std_per_patient_omr = omr_weights.groupby('subject_id')['patientweight_omr'].std()\n",
    "average_std_omr = std_per_patient_omr.mean()\n",
    "average_std_omr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_subjects = final_df[['subject_id']].drop_duplicates()\n",
    "merged_weights = unique_subjects.merge(omr_weights, on='subject_id', how='left')\n",
    "merged_weights.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the average standard deviation for weight across all patients\n",
    "std_per_patient_omr = merged_weights.groupby('subject_id')['patientweight_omr'].std()\n",
    "average_std_omr = std_per_patient_omr.mean()\n",
    "average_std_omr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the dataset to have only one row per subject_id\n",
    "avg_weight = omr_weights.groupby('subject_id', as_index=False)['patientweight_omr'].mean()\n",
    "avg_weight.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can merge it with your other dataset\n",
    "final_df = final_df.merge(avg_weight, on='subject_id', how='left')\n",
    "final_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the two weight sources: use omr weight if available\n",
    "final_df['patientweight'] = final_df['patientweight_omr'].combine_first(final_df['patientweight'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the helper column\n",
    "final_df.drop(columns=['patientweight_omr'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing connection\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the column hadm_id from admissions table and drop duplicated rows\n",
    "final_df.drop(columns=['hadm_id', 'admittime', 'dischtime'], inplace=True)\n",
    "final_df.drop_duplicates(inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
